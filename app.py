import os
os.environ["PYTHONASYNCIODEBUG"] = "0"
os.environ["UVICORN_LOOP"] = "asyncio"  

import sqlite3
from fastapi import FastAPI, UploadFile, File, Form, HTTPException, Request
from fastapi.responses import HTMLResponse, JSONResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from google.genai import Client
from google.genai.errors import APIError
import sys
from typing import Annotated, Optional
import shutil
import tempfile
import traceback
import json
from datetime import datetime
import logging
import base64
import re
import time
import pandas as pd
import io
import uuid

# ‚ö° Production-safe logging
logging.basicConfig(
    level=logging.WARNING,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

app = FastAPI()

def resource_path(relative_path):
    """Retourne le chemin correct m√™me dans un .exe"""
    if hasattr(sys, "_MEIPASS"):
        return os.path.join(sys._MEIPASS, relative_path)
    return os.path.join(os.path.abspath("."), relative_path)

# Mount static folder correctement
static_dir = resource_path("static")
app.mount("/static", StaticFiles(directory=static_dir), name="static")

# ---------------------
# Configuration du logging
# ---------------------
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ---------------------
# Configuration et Initialisation de l'API Gemini (GenAI)
# ---------------------
API_KEY_NAO = os.getenv("GEMINI_API_KEY", "AIzaSyCCnrruOeLHd5V4gKoDnhoKdXQThHqWKHs")

try:
    client = Client(api_key=API_KEY_NAO)
    logger.info("Client Gemini initialis√© avec succ√®s")
except Exception as e:
    logger.error(f"Erreur lors de l'initialisation de l'API: {e}")
    raise RuntimeError(f"Erreur lors de l'initialisation de l'API: {e}")

# ---------------------
# Cr√©ation de l'application FastAPI
# ---------------------
app = FastAPI(
    title="Swis Madagascar - Syst√®me d'Analyse Intelligente",
    description="Application de d√©tection automatique des anomalies financi√®res et de stock",
    version="5.0.0"
)

# ---------------------
# Configuration CORS
# ---------------------
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Configuration des dossiers
os.makedirs("static", exist_ok=True)
app.mount("/static", StaticFiles(directory="static"), name="static")

# --- Configuration SQLite (Threads & Messages) ---
DB_NAME = "analyse_db_thread.sqlite" 

def init_db():
    """Cr√©e la base de donn√©es et les tables n√©cessaires avec des index pour les performances."""
    conn = sqlite3.connect(DB_NAME)
    cursor = conn.cursor()
    
    # TABLE 1: THREADS (la conversation/analyse)
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS THREADS (
            thread_id INTEGER PRIMARY KEY AUTOINCREMENT,
            title TEXT NOT NULL,
            date_creation DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
            date_modification DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP
        )
    """)
    
    # TABLE 2: MESSAGES (les √©tapes de l'analyse)
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS MESSAGES (
            message_id INTEGER PRIMARY KEY AUTOINCREMENT,
            thread_id INTEGER NOT NULL,
            sender VARCHAR(50) NOT NULL, -- 'user' ou 'assistant'
            content TEXT NOT NULL,
            date_message DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
            statut VARCHAR(20) NOT NULL DEFAULT 'Succ√®s',
            FOREIGN KEY (thread_id) REFERENCES THREADS(thread_id) ON DELETE CASCADE
        )
    """)
    
    # TABLE 3: FICHIERS_MESSAGES 
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS FICHIERS_MESSAGES (
            fichier_message_id INTEGER PRIMARY KEY AUTOINCREMENT,
            message_id INTEGER NOT NULL,
            nom_fichier VARCHAR(255) NOT NULL,
            taille_ko INTEGER,
            type_mime VARCHAR(100),
            gemini_file_name VARCHAR(255) NOT NULL,
            file_content TEXT,
            FOREIGN KEY (message_id) REFERENCES MESSAGES(message_id) ON DELETE CASCADE
        )
    """)
    
    # TABLE 4: ANOMALIES_DETAILLEES (pour stocker les anomalies d√©tect√©es)
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS ANOMALIES_DETAILLEES (
            anomalie_id INTEGER PRIMARY KEY AUTOINCREMENT,
            thread_id INTEGER NOT NULL,
            fichier_source VARCHAR(255) NOT NULL,
            type_anomalie VARCHAR(100) NOT NULL,
            description TEXT NOT NULL,
            localisation VARCHAR(255),
            impact_estime DECIMAL(15,2),
            criticite VARCHAR(20),
            recommandation TEXT,
            date_detection DATETIME DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (thread_id) REFERENCES THREADS(thread_id) ON DELETE CASCADE
        )
    """)
    
    # TABLE 5: STATISTIQUES_FICHIERS (pour stocker les stats globales)
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS STATISTIQUES_FICHIERS (
            stat_id INTEGER PRIMARY KEY AUTOINCREMENT,
            thread_id INTEGER NOT NULL,
            nom_fichier VARCHAR(255) NOT NULL,
            type_fichier VARCHAR(100),
            nombre_lignes INTEGER,
            nombre_colonnes INTEGER,
            chiffre_affaires DECIMAL(15,2),
            nombre_transactions INTEGER,
            montant_reductions DECIMAL(15,2),
            donnees_manquantes INTEGER,
            FOREIGN KEY (thread_id) REFERENCES THREADS(thread_id) ON DELETE CASCADE
        )
    """)
    
    # Cr√©ation d'index pour am√©liorer les performances
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_messages_thread_id ON MESSAGES(thread_id)")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_messages_date ON MESSAGES(date_message)")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_threads_date_modification ON THREADS(date_modification)")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_anomalies_thread_id ON ANOMALIES_DETAILLEES(thread_id)")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_stats_thread_id ON STATISTIQUES_FICHIERS(thread_id)")
    
    conn.commit()
    conn.close()
    logger.info(f"Base de donn√©es '{DB_NAME}' initialis√©e.")

def update_db_schema():
    """Met √† jour le sch√©ma de la base de donn√©es si n√©cessaire."""
    conn = sqlite3.connect(DB_NAME)
    cursor = conn.cursor()
    
    try:
        # V√©rifier si la colonne file_content existe
        cursor.execute("PRAGMA table_info(FICHIERS_MESSAGES)")
        columns = [column[1] for column in cursor.fetchall()]
        
        if 'file_content' not in columns:
            logger.info("Ajout de la colonne file_content...")
            cursor.execute("ALTER TABLE FICHIERS_MESSAGES ADD COLUMN file_content TEXT")
            conn.commit()
            logger.info("Colonne file_content ajout√©e avec succ√®s.")
        else:
            logger.info("La colonne file_content existe d√©j√†.")
            
        # V√©rifier si la table ANOMALIES_DETAILLEES existe
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='ANOMALIES_DETAILLEES'")
        if not cursor.fetchone():
            logger.info("Cr√©ation de la table ANOMALIES_DETAILLEES...")
            cursor.execute("""
                CREATE TABLE ANOMALIES_DETAILLEES (
                    anomalie_id INTEGER PRIMARY KEY AUTOINCREMENT,
                    thread_id INTEGER NOT NULL,
                    fichier_source VARCHAR(255) NOT NULL,
                    type_anomalie VARCHAR(100) NOT NULL,
                    description TEXT NOT NULL,
                    localisation VARCHAR(255),
                    impact_estime DECIMAL(15,2),
                    criticite VARCHAR(20),
                    recommandation TEXT,
                    date_detection DATETIME DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (thread_id) REFERENCES THREADS(thread_id) ON DELETE CASCADE
                )
            """)
            cursor.execute("CREATE INDEX idx_anomalies_thread_id ON ANOMALIES_DETAILLEES(thread_id)")
            conn.commit()
            logger.info("Table ANOMALIES_DETAILLEES cr√©√©e avec succ√®s.")
            
        # V√©rifier si la table STATISTIQUES_FICHIERS existe
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='STATISTIQUES_FICHIERS'")
        if not cursor.fetchone():
            logger.info("Cr√©ation de la table STATISTIQUES_FICHIERS...")
            cursor.execute("""
                CREATE TABLE STATISTIQUES_FICHIERS (
                    stat_id INTEGER PRIMARY KEY AUTOINCREMENT,
                    thread_id INTEGER NOT NULL,
                    nom_fichier VARCHAR(255) NOT NULL,
                    type_fichier VARCHAR(100),
                    nombre_lignes INTEGER,
                    nombre_colonnes INTEGER,
                    chiffre_affaires DECIMAL(15,2),
                    nombre_transactions INTEGER,
                    montant_reductions DECIMAL(15,2),
                    donnees_manquantes INTEGER,
                    FOREIGN KEY (thread_id) REFERENCES THREADS(thread_id) ON DELETE CASCADE
                )
            """)
            cursor.execute("CREATE INDEX idx_stats_thread_id ON STATISTIQUES_FICHIERS(thread_id)")
            conn.commit()
            logger.info("Table STATISTIQUES_FICHIERS cr√©√©e avec succ√®s.")
            
    except Exception as e:
        logger.error(f"Erreur lors de la mise √† jour du sch√©ma: {e}")
    finally:
        conn.close()

# Initialisation de la base de donn√©es
init_db()
update_db_schema()

# --- Fonctions utilitaires am√©lior√©es ---

def get_db_connection():
    """Retourne une connexion √† la base de donn√©es avec gestion d'erreurs."""
    try:
        conn = sqlite3.connect(DB_NAME)
        conn.row_factory = sqlite3.Row
        return conn
    except sqlite3.Error as e:
        logger.error(f"Erreur de connexion √† la base de donn√©es: {e}")
        raise

def _create_new_thread(cursor, initial_prompt):
    """Cr√©e un nouveau thread avec un titre bas√© sur le prompt."""
    title = (initial_prompt[:50] + '...') if len(initial_prompt) > 50 else initial_prompt
    cursor.execute(
        "INSERT INTO THREADS (title) VALUES (?)", 
        (title,)
    )
    thread_id = cursor.lastrowid
    logger.info(f"Nouveau thread cr√©√©: {thread_id}")
    return thread_id

def _update_thread_date(cursor, thread_id):
    """Met √† jour la date de modification du thread."""
    cursor.execute(
        "UPDATE THREADS SET date_modification = CURRENT_TIMESTAMP WHERE thread_id = ?", 
        (thread_id,)
    )

def _save_message(cursor, thread_id, sender, content, status='Succ√®s'):
    """Enregistre un message (user ou assistant) dans le thread."""
    cursor.execute(
        "INSERT INTO MESSAGES (thread_id, sender, content, statut) VALUES (?, ?, ?, ?)",
        (thread_id, sender, content, status)
    )
    message_id = cursor.lastrowid
    logger.debug(f"Message sauvegard√©: {message_id} pour le thread {thread_id}")
    return message_id

def _save_files_to_message(cursor, message_id, file_infos_for_db):
    """Enregistre les fichiers li√©s √† un message."""
    for f_info in file_infos_for_db:
        cursor.execute(
            """INSERT INTO FICHIERS_MESSAGES 
               (message_id, nom_fichier, taille_ko, type_mime, gemini_file_name, file_content) 
               VALUES (?, ?, ?, ?, ?, ?)""",
            (message_id, f_info['display_name'], f_info['size_ko'], 
             f_info['mime_type'], f_info['gemini_file_name'], f_info.get('file_content', ''))
        )

def _save_detailed_anomalies(cursor, thread_id, anomalies_detailed):
    """Enregistre les anomalies d√©taill√©es dans la base de donn√©es."""
    for anomaly in anomalies_detailed:
        cursor.execute(
            """INSERT INTO ANOMALIES_DETAILLEES 
               (thread_id, fichier_source, type_anomalie, description, localisation, impact_estime, criticite, recommandation)
               VALUES (?, ?, ?, ?, ?, ?, ?, ?)""",
            (thread_id, anomaly['fichier_source'], anomaly['type_anomalie'], 
             anomaly['description'], anomaly.get('localisation'), anomaly.get('impact_estime'),
             anomaly.get('criticite'), anomaly.get('recommandation'))
        )

def _save_file_statistics(cursor, thread_id, file_stats):
    """Enregistre les statistiques des fichiers analys√©s."""
    for stat in file_stats:
        cursor.execute(
            """INSERT INTO STATISTIQUES_FICHIERS 
               (thread_id, nom_fichier, type_fichier, nombre_lignes, nombre_colonnes, 
                chiffre_affaires, nombre_transactions, montant_reductions, donnees_manquantes)
               VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)""",
            (thread_id, stat['nom_fichier'], stat['type_fichier'], stat['nombre_lignes'], 
             stat['nombre_colonnes'], stat.get('chiffre_affaires'), stat.get('nombre_transactions'),
             stat.get('montant_reductions'), stat.get('donnees_manquantes'))
        )

def _cleanup_files(gemini_uploaded_files, temp_file_paths):
    """Nettoie les fichiers temporaires et les fichiers Gemini."""
    for f in gemini_uploaded_files:
        try:
            client.files.delete(name=f.name)
            logger.debug(f"Fichier Gemini supprim√©: {f.name}")
        except Exception as e:
            logger.warning(f"Erreur lors de la suppression du fichier Gemini {f.name}: {e}")
    
    for temp_file_path in temp_file_paths:
        if os.path.exists(temp_file_path):
            try:
                os.remove(temp_file_path)
                logger.debug(f"Fichier temporaire supprim√©: {temp_file_path}")
            except Exception as e:
                logger.warning(f"Erreur lors de la suppression du fichier temporaire {temp_file_path}: {e}")

def extract_anomaly_stats(response_text):
    """Extrait les statistiques d'anomalies de la r√©ponse."""
    try:
        total_anomalies = len(re.findall(r'üö®\s*\*\*ANOMALIE D√âTECT√âE\*\*', response_text, re.IGNORECASE))
        financial_anomalies = len(re.findall(r'ANOMALIES FINANCI√àRES', response_text, re.IGNORECASE))
        stock_anomalies = len(re.findall(r'ERREURS DE STOCK', response_text, re.IGNORECASE))
        pricing_anomalies = len(re.findall(r'ANOMALIES DE TARIFICATION', response_text, re.IGNORECASE))
        
        impact_amounts = re.findall(r'üí∞ Impact :.*?(\d+[\d\s,]*\.?\d*)\s*(MGA|‚Ç¨|euros?|ariary)', response_text, re.IGNORECASE)
        total_impact = 0
        for amount, currency in impact_amounts:
            try:
                clean_amount = amount.replace(' ', '').replace(',', '.')
                total_impact += float(clean_amount)
            except ValueError:
                continue
        
        has_critical_issues = total_anomalies > 0
        
        return {
            "total_anomalies": total_anomalies,
            "financial_anomalies": financial_anomalies,
            "stock_anomalies": stock_anomalies,
            "pricing_anomalies": pricing_anomalies,
            "total_impact": round(total_impact, 2),
            "has_critical_issues": has_critical_issues,
            "impact_currency": "MGA" if impact_amounts else "N/A"
        }
    except Exception as e:
        logger.error(f"Erreur lors de l'extraction des stats d'anomalies: {e}")
        return {
            "total_anomalies": 0,
            "financial_anomalies": 0,
            "stock_anomalies": 0,
            "pricing_anomalies": 0,
            "total_impact": 0,
            "has_critical_issues": False,
            "impact_currency": "N/A"
        }

def extract_detailed_anomalies(response_text, file_names):
    """Extrait les anomalies d√©taill√©es de la r√©ponse pour un stockage structur√©."""
    try:
        anomalies = []
        
        # Pattern pour d√©tecter chaque anomalie
        anomaly_pattern = r'üö®\s*\*\*ANOMALIE D√âTECT√âE\*\*(.*?)(?=üö®\s*\*\*ANOMALIE D√âTECT√âE\*\*|üîÑ\s*\*\*√âTAPE|üìä\s*\*\*R√âSUM√â|$)'
        matches = re.findall(anomaly_pattern, response_text, re.DOTALL | re.IGNORECASE)
        
        for match in matches:
            anomaly_text = match.strip()
            
            # Extraire les informations structur√©es
            fichier_match = re.search(r'üìÅ\s*Fichier:\s*(.*?)(?=\n|$)', anomaly_text, re.IGNORECASE)
            localisation_match = re.search(r'üìç\s*Localisation:\s*(.*?)(?=\n|$)', anomaly_text, re.IGNORECASE)
            description_match = re.search(r'üîé\s*Description:\s*(.*?)(?=\n|$)', anomaly_text, re.IGNORECASE)
            impact_match = re.search(r'üí∞\s*Impact:\s*(.*?)(?=\n|$)', anomaly_text, re.IGNORECASE)
            recommandation_match = re.search(r'‚úÖ\s*Recommandation:\s*(.*?)(?=\n|$)', anomaly_text, re.IGNORECASE)
            
            # D√©terminer le type d'anomalie
            type_anomalie = "Autre"
            if re.search(r'financier|argent|co√ªt|prix|euro|ariary|mga', anomaly_text, re.IGNORECASE):
                type_anomalie = "Financi√®re"
            elif re.search(r'stock|inventaire|quantit√©|produit', anomaly_text, re.IGNORECASE):
                type_anomalie = "Stock"
            elif re.search(r'tarification|prix|co√ªt', anomaly_text, re.IGNORECASE):
                type_anomalie = "Tarification"
            
            # D√©terminer la criticit√©
            criticite = "Moyenne"
            if re.search(r'critique|urgent|grave|important', anomaly_text, re.IGNORECASE):
                criticite = "√âlev√©e"
            elif re.search(r'mineur|faible|petit', anomaly_text, re.IGNORECASE):
                criticite = "Faible"
            
            # Extraire l'impact num√©rique
            impact_estime = 0
            if impact_match:
                impact_text = impact_match.group(1)
                montant_match = re.search(r'(\d+[\d\s,]*\.?\d*)', impact_text)
                if montant_match:
                    try:
                        montant = montant_match.group(1).replace(' ', '').replace(',', '.')
                        impact_estime = float(montant)
                    except ValueError:
                        pass
            
            anomalies.append({
                "fichier_source": fichier_match.group(1).strip() if fichier_match else file_names[0] if file_names else "Fichier inconnu",
                "type_anomalie": type_anomalie,
                "description": description_match.group(1).strip() if description_match else "Description non sp√©cifi√©e",
                "localisation": localisation_match.group(1).strip() if localisation_match else "Localisation non sp√©cifi√©e",
                "impact_estime": impact_estime,
                "criticite": criticite,
                "recommandation": recommandation_match.group(1).strip() if recommandation_match else "Recommandation non sp√©cifi√©e"
            })
        
        return anomalies
    except Exception as e:
        logger.error(f"Erreur lors de l'extraction des anomalies d√©taill√©es: {e}")
        return []

def analyze_file_statistics(file_path, mime_type, filename):
    """Analyse les statistiques de base d'un fichier."""
    try:
        stats = {
            "nom_fichier": filename,
            "type_fichier": mime_type,
            "nombre_lignes": 0,
            "nombre_colonnes": 0,
            "chiffre_affaires": 0,
            "nombre_transactions": 0,
            "montant_reductions": 0,
            "donnees_manquantes": 0
        }
        
        if mime_type in ['application/vnd.ms-excel', 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet']:
            df = pd.read_excel(file_path)
            stats["nombre_lignes"] = len(df)
            stats["nombre_colonnes"] = len(df.columns)
            stats["donnees_manquantes"] = df.isnull().sum().sum()
            
            # Tentative d'identification des colonnes financi√®res
            for col in df.columns:
                if any(keyword in str(col).lower() for keyword in ['montant', 'prix', 'total', 'chiffre', 'affaires', 'ca']):
                    if pd.api.types.is_numeric_dtype(df[col]):
                        stats["chiffre_affaires"] = df[col].sum()
                elif any(keyword in str(col).lower() for keyword in ['quantit√©', 'qte', 'nombre']):
                    if pd.api.types.is_numeric_dtype(df[col]):
                        stats["nombre_transactions"] = df[col].sum()
                elif any(keyword in str(col).lower() for keyword in ['r√©duction', 'remise', 'discount']):
                    if pd.api.types.is_numeric_dtype(df[col]):
                        stats["montant_reductions"] = df[col].sum()
                        
        elif mime_type == 'text/csv':
            df = pd.read_csv(file_path, encoding='utf-8', errors='ignore')
            stats["nombre_lignes"] = len(df)
            stats["nombre_colonnes"] = len(df.columns)
            stats["donnees_manquantes"] = df.isnull().sum().sum()
            
            # Tentative d'identification des colonnes financi√®res
            for col in df.columns:
                if any(keyword in str(col).lower() for keyword in ['montant', 'prix', 'total', 'chiffre', 'affaires', 'ca']):
                    try:
                        df[col] = pd.to_numeric(df[col], errors='coerce')
                        stats["chiffre_affaires"] = df[col].sum()
                    except:
                        pass
                elif any(keyword in str(col).lower() for keyword in ['quantit√©', 'qte', 'nombre']):
                    try:
                        df[col] = pd.to_numeric(df[col], errors='coerce')
                        stats["nombre_transactions"] = df[col].sum()
                    except:
                        pass
                elif any(keyword in str(col).lower() for keyword in ['r√©duction', 'remise', 'discount']):
                    try:
                        df[col] = pd.to_numeric(df[col], errors='coerce')
                        stats["montant_reductions"] = df[col].sum()
                    except:
                        pass
        
        return stats
        
    except Exception as e:
        logger.error(f"Erreur lors de l'analyse des statistiques du fichier {filename}: {e}")
        return {
            "nom_fichier": filename,
            "type_fichier": mime_type,
            "nombre_lignes": 0,
            "nombre_colonnes": 0,
            "chiffre_affaires": 0,
            "nombre_transactions": 0,
            "montant_reductions": 0,
            "donnees_manquantes": 0
        }

async def call_gemini_api_with_retry(contents, max_retries=3):
    """Effectue l'appel API avec syst√®me de retry et gestion d'erreurs."""
    
    models_to_try = [
        'gemini-2.0-flash-exp',
        'gemini-1.5-flash', 
        'gemini-1.5-pro'
    ]
    
    for attempt in range(max_retries):
        for model in models_to_try:
            try:
                logger.info(f"Tentative {attempt + 1} avec mod√®le: {model}")
                
                api_response = client.models.generate_content(
                    model=model,
                    contents=contents,
                    config={
                        'temperature': 0.1,
                        'top_p': 0.8,
                        'top_k': 40,
                        'max_output_tokens': 4000
                    }
                )
                
                logger.info(f"‚úÖ Succ√®s avec le mod√®le: {model}")
                return api_response.text, "Succ√®s"
                
            except APIError as e:
                error_msg = str(e).lower()
                if 'overload' in error_msg or 'unavailable' in error_msg or '503' in str(e):
                    logger.warning(f"‚ö†Ô∏è Mod√®le {model} surcharg√©, tentative suivante...")
                    continue
                else:
                    logger.error(f"‚ùå Erreur API avec {model}: {e}")
                    return f"Erreur technique: {str(e)}", "Erreur Technique"
                    
            except Exception as e:
                logger.error(f"‚ùå Erreur inattendue avec {model}: {e}")
                continue
        
        if attempt < max_retries - 1:
            wait_time = (attempt + 1) * 3
            logger.info(f"‚è≥ Attente de {wait_time}s avant nouvelle tentative...")
            time.sleep(wait_time)
    
    error_msg = """üîß **Service Temporairement Indisponible**

Nous rencontrons actuellement une forte demande sur notre service d'analyse.

üí° **Que faire ?**
‚Ä¢ R√©essayez dans 2-3 minutes
‚Ä¢ V√©rifiez votre connexion internet
‚Ä¢ R√©duisez le nombre de fichiers si possible

üìû **Assistance**
Si le probl√®me persiste, contactez notre support technique.

Nous vous remercions de votre patience."""
    
    return error_msg, "Service Indisponible"

def read_file_content(file_path, mime_type):
    """Lit le contenu d'un fichier selon son type avec un meilleur formatage."""
    try:
        if mime_type in ['application/vnd.ms-excel', 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet']:
            try:
                # Lire le fichier Excel
                df = pd.read_excel(file_path, nrows=100)
                
                # Obtenir les informations de base
                file_info = f"üìä FICHIER EXCEL: {os.path.basename(file_path)}\n"
                file_info += f"üìè Dimensions: {df.shape[0]} lignes √ó {df.shape[1]} colonnes\n"
                file_info += f"üìã Colonnes: {', '.join(df.columns.astype(str))}\n\n"
                
                # Informations sur les types de donn√©es
                file_info += "üîç TYPES DE DONN√âES PAR COLONNE:\n"
                for col in df.columns:
                    dtype = str(df[col].dtype)
                    non_null = df[col].count()
                    total = len(df[col])
                    file_info += f"   ‚Ä¢ {col}: {dtype} ({non_null}/{total} non-null)\n"
                
                file_info += f"\nüìã APER√áU DES DONN√âES (premi√®res 20 lignes):\n"
                file_info += "‚îÄ" * 80 + "\n"
                
                # Formater l'aper√ßu des donn√©es de mani√®re plus lisible
                preview_df = df.head(20)
                
                # Cr√©er une repr√©sentation textuelle format√©e
                with pd.option_context('display.max_rows', 20, 'display.max_columns', 10, 'display.width', 1000):
                    file_info += preview_df.to_string(index=False)
                
                file_info += f"\n\nüìà STATISTIQUES NUM√âRIQUES:\n"
                numeric_cols = df.select_dtypes(include=['number']).columns
                if len(numeric_cols) > 0:
                    file_info += df[numeric_cols].describe().to_string()
                else:
                    file_info += "   Aucune colonne num√©rique trouv√©e"
                
                return file_info
                
            except Exception as e:
                return f"üìä FICHIER EXCEL: {os.path.basename(file_path)}\n‚ùå Erreur de lecture: {str(e)}"
                
        elif mime_type == 'text/csv':
            try:
                # Lire le fichier CSV
                df = pd.read_csv(file_path, nrows=100, encoding='utf-8', errors='ignore')
                
                # Obtenir les informations de base
                file_info = f"üìÑ FICHIER CSV: {os.path.basename(file_path)}\n"
                file_info += f"üìè Dimensions: {df.shape[0]} lignes √ó {df.shape[1]} colonnes\n"
                file_info += f"üìã Colonnes: {', '.join(df.columns.astype(str))}\n\n"
                
                # Informations sur les types de donn√©es
                file_info += "üîç TYPES DE DONN√âES PAR COLONNE:\n"
                for col in df.columns:
                    dtype = str(df[col].dtype)
                    non_null = df[col].count()
                    total = len(df[col])
                    file_info += f"   ‚Ä¢ {col}: {dtype} ({non_null}/{total} non-null)\n"
                
                file_info += f"\nüìã APER√áU DES DONN√âES (premi√®res 20 lignes):\n"
                file_info += "‚îÄ" * 80 + "\n"
                
                # Formater l'aper√ßu des donn√©es
                preview_df = df.head(20)
                
                with pd.option_context('display.max_rows', 20, 'display.max_columns', 10, 'display.width', 1000):
                    file_info += preview_df.to_string(index=False)
                
                file_info += f"\n\nüìà STATISTIQUES NUM√âRIQUES:\n"
                numeric_cols = df.select_dtypes(include=['number']).columns
                if len(numeric_cols) > 0:
                    file_info += df[numeric_cols].describe().to_string()
                else:
                    file_info += "   Aucune colonne num√©rique trouv√©e"
                
                return file_info
                
            except Exception as e:
                return f"üìÑ FICHIER CSV: {os.path.basename(file_path)}\n‚ùå Erreur de lecture: {str(e)}"
                
        elif mime_type == 'application/pdf':
            return f"üìë FICHIER PDF: {os.path.basename(file_path)}\nüìè Taille: {os.path.getsize(file_path)/1024:.2f} KB\nüí° Contenu: Document PDF (analyse textuelle limit√©e)"
        
        elif 'text' in mime_type:
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                    
                file_info = f"üìù FICHIER TEXTE: {os.path.basename(file_path)}\n"
                file_info += f"üìè Taille: {len(content)} caract√®res\n"
                file_info += f"üìä Lignes: {len(content.splitlines())}\n\n"
                
                # Aper√ßu du contenu
                preview_lines = content.splitlines()[:30]
                file_info += "üìã APER√áU DU CONTENU:\n"
                file_info += "‚îÄ" * 80 + "\n"
                file_info += "\n".join(preview_lines)
                
                if len(content.splitlines()) > 30:
                    file_info += f"\n[...] {len(content.splitlines()) - 30} lignes suppl√©mentaires"
                
                return file_info
                
            except Exception as e:
                return f"üìù FICHIER TEXTE: {os.path.basename(file_path)}\n‚ùå Erreur de lecture: {str(e)}"
                
        else:
            return f"üìÅ FICHIER: {os.path.basename(file_path)}\nüî§ Type: {mime_type}\nüìè Taille: {os.path.getsize(file_path)/1024:.2f} KB\nüí° Type non support√© pour l'aper√ßu d√©taill√©"
            
    except Exception as e:
        logger.error(f"Erreur lecture fichier {file_path}: {e}")
        return f"‚ùå Erreur lors de la lecture du fichier: {str(e)}"

# --- Endpoints principaux ---

@app.get("/", response_class=HTMLResponse)
async def serve_frontend():
    """Sert le fichier HTML principal."""
    try:
        with open("static/index.html", "r", encoding="utf-8") as f:
            return f.read()
    except FileNotFoundError:
        logger.error("Fichier static/index.html introuvable")
        return HTMLResponse(
            "<h1>Erreur: Fichier 'static/index.html' introuvable.</h1>", 
            status_code=404
        )

@app.get("/api/health")
async def health_check():
    """Endpoint de sant√© de l'application."""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "version": "5.0.0"
    }

@app.post("/api/process_query")
async def process_multimodal_query(
    prompt: Annotated[str, Form(description="La question √† poser")] = "",
    thread_id: Annotated[Optional[int], Form(description="ID du thread pour la conversation continue")] = None,
    files: Annotated[list[UploadFile] | None, File(description="Liste de fichiers optionnels")] = None,
):
    """Traite la requ√™te multimodale avec gestion robuste des erreurs et analyse structur√©e."""
    
    if not files or len(files) == 0:
        raise HTTPException(
            status_code=400,
            detail="Veuillez s√©lectionner au moins un fichier √† analyser."
        )

    # Variables de traitement
    gemini_uploaded_files = []
    files_info_for_db = []
    temp_file_paths = []
    response_text = ""
    final_status = "Succ√®s"
    anomaly_stats = {}
    anomalies_detailed = []
    file_statistics = []
    
    conn = None
    user_message_id = None
    
    try:
        contents = []
        conn = get_db_connection()
        cursor = conn.cursor()

        # 1. Gestion du Thread
        file_names = ", ".join([file.filename for file in files if file.filename])
        prompt_for_title = f"Analyse: {file_names}" if file_names else "Analyse automatique"
        
        if thread_id is None or thread_id == 0:
            thread_id = _create_new_thread(cursor, prompt_for_title)
        else:
            _update_thread_date(cursor, thread_id)
            
        # 2. Enregistrement du message utilisateur
        user_message_id = _save_message(cursor, thread_id, 'user', "Analyse automatique des fichiers d√©pos√©s", 'En cours')
        
        # 3. Traitement des fichiers
        total_file_size = 0
        max_file_size = 20 * 1024 * 1024
        
        file_details = []
        file_names_list = []
        
        for file in files:
            if not file.filename:
                continue
                
            file_size = file.size or 0
            total_file_size += file_size
            
            if total_file_size > max_file_size:
                raise HTTPException(
                    status_code=400,
                    detail=f"Taille totale des fichiers trop importante ({total_file_size/1024/1024:.1f}MB). Maximum: 20MB"
                )

            # Upload vers Gemini
            temp_file_path = None
            try:
                with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file.filename)[1]) as temp:
                    content = await file.read()
                    temp.write(content)
                    temp_file_path = temp.name
                    temp_file_paths.append(temp_file_path)
                
                await file.seek(0)
                    
                uploaded_file_gemini = client.files.upload(file=temp_file_path)
                
                if uploaded_file_gemini:
                    gemini_uploaded_files.append(uploaded_file_gemini)
                    contents.append(uploaded_file_gemini)
                    
                    # Sauvegarder le contenu du fichier pour l'aper√ßu
                    file_content = read_file_content(temp_file_path, uploaded_file_gemini.mime_type)
                    
                    files_info_for_db.append({
                        'display_name': file.filename,
                        'size_ko': file_size / 1024,
                        'mime_type': uploaded_file_gemini.mime_type,
                        'gemini_file_name': uploaded_file_gemini.name,
                        'file_content': file_content
                    })
                    
                    file_details.append(f"üìÑ {file.filename} ({(file_size/1024/1024):.2f} MB)")
                    file_names_list.append(file.filename)
                    
                    # Analyser les statistiques du fichier
                    file_stats = analyze_file_statistics(temp_file_path, uploaded_file_gemini.mime_type, file.filename)
                    file_statistics.append(file_stats)
                    
            except Exception as e:
                logger.error(f"Erreur upload fichier {file.filename}: {e}")
                final_status = "Erreur Fichier"
                continue
        
        # Sauvegarde des infos fichiers
        if files_info_for_db:
            _save_files_to_message(cursor, user_message_id, files_info_for_db)
            
        # Sauvegarde des statistiques
        if file_statistics:
            _save_file_statistics(cursor, thread_id, file_statistics)
    
        # 4. Pr√©paration du contenu avec la nouvelle structure d'analyse simplifi√©e
        file_list_text = "\n".join(file_details)
        
        # Pr√©parer le r√©sum√© des statistiques
        stats_summary = "üìä R√âSUM√â GLOBAL DES FICHIERS ANALYS√âS:\n\n"
        total_ca = 0
        total_transactions = 0
        total_reductions = 0
        
        for stat in file_statistics:
            stats_summary += f"üìÅ {stat['nom_fichier']}:\n"
            stats_summary += f"   ‚Ä¢ Lignes: {stat['nombre_lignes']:,}\n"
            stats_summary += f"   ‚Ä¢ Colonnes: {stat['nombre_colonnes']}\n"
            if stat['chiffre_affaires'] > 0:
                stats_summary += f"   ‚Ä¢ Chiffre d'affaires: {stat['chiffre_affaires']:,.2f} MGA\n"
                total_ca += stat['chiffre_affaires']
            if stat['nombre_transactions'] > 0:
                stats_summary += f"   ‚Ä¢ Transactions: {stat['nombre_transactions']:,}\n"
                total_transactions += stat['nombre_transactions']
            if stat['montant_reductions'] > 0:
                stats_summary += f"   ‚Ä¢ R√©ductions: {stat['montant_reductions']:,.2f} MGA\n"
                total_reductions += stat['montant_reductions']
            if stat['donnees_manquantes'] > 0:
                stats_summary += f"   ‚Ä¢ Donn√©es manquantes: {stat['donnees_manquantes']}\n"
            stats_summary += "\n"
        
        stats_summary += f"üìà TOTAUX GLOBAUX:\n"
        stats_summary += f"   ‚Ä¢ Chiffre d'affaires total: {total_ca:,.2f} MGA\n"
        stats_summary += f"   ‚Ä¢ Nombre total de transactions: {total_transactions:,}\n"
        stats_summary += f"   ‚Ä¢ Montant total des r√©ductions: {total_reductions:,.2f} MGA\n"
        
        analysis_prompt = f"""
ANALYSE SWIS MADAGASCAR - RAPPORT COMPLET

{stats_summary}

üîç STRUCTURE LOGIQUE DE L'ANALYSE SIMPLIFI√âE :

üéØ √âTAPE 1 - COLLECTE DES FICHIERS
‚Ä¢ Recherche, chargement et pr√©paration de tous les fichiers n√©cessaires
‚Ä¢ V√©rification de l'int√©grit√© et de la compl√©tude des donn√©es

üéØ √âTAPE 2 - V√âRIFICATION INTERNE  
‚Ä¢ Analyse individuelle de chaque fichier
‚Ä¢ D√©tection des erreurs internes : valeurs manquantes, doublons, incoh√©rences locales
‚Ä¢ Identification des anomalies structurelles

üéØ √âTAPE 3 - V√âRIFICATION CROIS√âE
‚Ä¢ Comparaison des fichiers entre eux
‚Ä¢ Rep√©rage des diff√©rences et contradictions dans les donn√©es communes
‚Ä¢ Identification des √©carts inter-fichiers

üéØ √âTAPE 4 - INTERPR√âTATION
‚Ä¢ √âvaluation de la gravit√© des anomalies d√©tect√©es
‚Ä¢ Analyse des causes racines possibles
‚Ä¢ Estimation de l'impact sur la fiabilit√© des donn√©es

üéØ √âTAPE 5 - RECOMMANDATIONS
‚Ä¢ Formulation de propositions concr√®tes pour corriger les anomalies
‚Ä¢ Suggestions d'am√©lioration pour la coh√©rence future
‚Ä¢ Hi√©rarchisation des actions par criticit√©

üéØ √âTAPE 6 - RAPPORT FINAL
‚Ä¢ G√©n√©ration d'un r√©sum√© clair et structur√©
‚Ä¢ Pr√©sentation des r√©sultats et du taux de conformit√© global
‚Ä¢ Identification des axes d'am√©lioration prioritaires

üìä FORMAT DE RAPPORT OBLIGATOIRE :

üö® **ANOMALIE D√âTECT√âE**
üìÅ Fichier: [Nom du fichier concern√©]
üìç Localisation: [Ligne/Colonne/Zone pr√©cise]
üîé Description: [Description d√©taill√©e du probl√®me]
üí∞ Impact: [Montant estim√© en MGA ou quantit√©]
üéØ Cause: [Analyse de la cause racine]
‚úÖ Recommandation: [Solution corrective concr√®te]

CRIT√àRES DE CRITICIT√â:
‚Ä¢ üî¥ CRITIQUE: Impact financier > 1,000,000 MGA ou risque op√©rationnel grave
‚Ä¢ üü° MOYEN: Impact entre 100,000 et 1,000,000 MGA
‚Ä¢ üü¢ FAIBLE: Impact < 100,000 MGA ou anomalie mineure

PR√âSENTEZ LES R√âSULTATS PAR ORDRE DE CRITICIT√â D√âCROISSANTE.
"""
        
        contents.append(analysis_prompt)
        
        if not contents:
            raise HTTPException(status_code=400, detail="Aucun contenu valide pour l'analyse.")

        # 5. Appel API avec gestion robuste
        response_text, api_status = await call_gemini_api_with_retry(contents)
        
        if api_status != "Succ√®s":
            final_status = api_status
        else:
            # Extraction des statistiques d'anomalies
            anomaly_stats = extract_anomaly_stats(response_text)
            
            # Extraction des anomalies d√©taill√©es pour le stockage structur√©
            anomalies_detailed = extract_detailed_anomalies(response_text, file_names_list)
            
            # Sauvegarde des anomalies d√©taill√©es dans la base de donn√©es
            if anomalies_detailed:
                _save_detailed_anomalies(cursor, thread_id, anomalies_detailed)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur interne: {traceback.format_exc()}")
        final_status = "Erreur Interne"
        response_text = f"Une erreur inattendue s'est produite. Veuillez r√©essayer."
        
    finally:
        # 6. Sauvegarde finale
        try:
            if conn:
                # Sauvegarde r√©ponse
                _save_message(cursor, thread_id, 'assistant', response_text, final_status)
                
                # Mise √† jour statut message utilisateur
                if user_message_id:
                    cursor.execute(
                        "UPDATE MESSAGES SET statut = ? WHERE message_id = ?",
                        (final_status, user_message_id)
                    )
                
                conn.commit()
                logger.info(f"Traitement termin√© - Thread: {thread_id}, Statut: {final_status}")

        except Exception as db_e:
            logger.error(f"Erreur BD finale: {db_e}")
            
        finally:
            # Nettoyage
            _cleanup_files(gemini_uploaded_files, temp_file_paths)
            if conn:
                conn.close()

    if final_status != "Succ√®s":
        raise HTTPException(
            status_code=500, 
            detail=response_text if "Erreur technique" not in response_text else "Probl√®me de connexion au service d'analyse"
        )

    return {
        "thread_id": thread_id, 
        "response": response_text,
        "status": final_status,
        "anomaly_stats": anomaly_stats,
        "anomalies_detailed": anomalies_detailed,
        "file_statistics": file_statistics
    }

# --- Nouveaux endpoints pour les donn√©es enrichies ---

@app.get("/api/thread/{thread_id}/anomalies")
async def get_thread_anomalies(thread_id: int):
    """R√©cup√®re les anomalies d√©taill√©es pour un thread sp√©cifique."""
    conn = None
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT 
                anomalie_id,
                fichier_source,
                type_anomalie,
                description,
                localisation,
                impact_estime,
                criticite,
                recommandation,
                date_detection
            FROM ANOMALIES_DETAILLEES
            WHERE thread_id = ?
            ORDER BY 
                CASE criticite
                    WHEN '√âlev√©e' THEN 1
                    WHEN 'Moyenne' THEN 2
                    WHEN 'Faible' THEN 3
                    ELSE 4
                END,
                impact_estime DESC
        """, (thread_id,))
        
        anomalies = []
        for row in cursor.fetchall():
            anomalies.append({
                "id": row["anomalie_id"],
                "fichier_source": row["fichier_source"],
                "type_anomalie": row["type_anomalie"],
                "description": row["description"],
                "localisation": row["localisation"],
                "impact_estime": float(row["impact_estime"]) if row["impact_estime"] else 0,
                "criticite": row["criticite"],
                "recommandation": row["recommandation"],
                "date_detection": row["date_detection"]
            })
        
        # Calcul des statistiques
        total_anomalies = len(anomalies)
        anomalies_critiques = len([a for a in anomalies if a["criticite"] == "√âlev√©e"])
        anomalies_moyennes = len([a for a in anomalies if a["criticite"] == "Moyenne"])
        anomalies_faibles = len([a for a in anomalies if a["criticite"] == "Faible"])
        impact_total = sum(a["impact_estime"] for a in anomalies)
        
        return {
            "thread_id": thread_id,
            "anomalies": anomalies,
            "statistiques": {
                "total_anomalies": total_anomalies,
                "anomalies_critiques": anomalies_critiques,
                "anomalies_moyennes": anomalies_moyennes,
                "anomalies_faibles": anomalies_faibles,
                "impact_total": round(impact_total, 2),
                "impact_moyen": round(impact_total / total_anomalies, 2) if total_anomalies > 0 else 0
            }
        }
        
    except Exception as e:
        logger.error(f"Erreur r√©cup√©ration anomalies thread {thread_id}: {e}")
        raise HTTPException(status_code=500, detail="Erreur lors de la r√©cup√©ration des anomalies.")
    finally:
        if conn:
            conn.close()

@app.get("/api/thread/{thread_id}/statistics")
async def get_thread_statistics(thread_id: int):
    """R√©cup√®re les statistiques des fichiers pour un thread sp√©cifique."""
    conn = None
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT 
                nom_fichier,
                type_fichier,
                nombre_lignes,
                nombre_colonnes,
                chiffre_affaires,
                nombre_transactions,
                montant_reductions,
                donnees_manquantes
            FROM STATISTIQUES_FICHIERS
            WHERE thread_id = ?
            ORDER BY nom_fichier
        """, (thread_id,))
        
        statistics = []
        for row in cursor.fetchall():
            statistics.append({
                "nom_fichier": row["nom_fichier"],
                "type_fichier": row["type_fichier"],
                "nombre_lignes": row["nombre_lignes"],
                "nombre_colonnes": row["nombre_colonnes"],
                "chiffre_affaires": float(row["chiffre_affaires"]) if row["chiffre_affaires"] else 0,
                "nombre_transactions": row["nombre_transactions"] or 0,
                "montant_reductions": float(row["montant_reductions"]) if row["montant_reductions"] else 0,
                "donnees_manquantes": row["donnees_manquantes"] or 0
            })
        
        # Calcul des totaux
        total_files = len(statistics)
        total_lignes = sum(s["nombre_lignes"] for s in statistics)
        total_ca = sum(s["chiffre_affaires"] for s in statistics)
        total_transactions = sum(s["nombre_transactions"] for s in statistics)
        total_reductions = sum(s["montant_reductions"] for s in statistics)
        total_manquantes = sum(s["donnees_manquantes"] for s in statistics)
        
        return {
            "thread_id": thread_id,
            "statistics": statistics,
            "totals": {
                "total_files": total_files,
                "total_lignes": total_lignes,
                "total_ca": round(total_ca, 2),
                "total_transactions": total_transactions,
                "total_reductions": round(total_reductions, 2),
                "total_manquantes": total_manquantes
            }
        }
        
    except Exception as e:
        logger.error(f"Erreur r√©cup√©ration statistiques thread {thread_id}: {e}")
        raise HTTPException(status_code=500, detail="Erreur lors de la r√©cup√©ration des statistiques.")
    finally:
        if conn:
            conn.close()

# --- Endpoints historiques ---

@app.get("/api/history")
async def get_history(limit: int = 50, offset: int = 0):
    """R√©cup√®re la liste des threads avec pagination."""
    conn = None
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT 
                thread_id, 
                title, 
                date_modification,
                (SELECT COUNT(*) FROM MESSAGES WHERE thread_id = THREADS.thread_id) as message_count,
                (SELECT statut FROM MESSAGES WHERE thread_id = THREADS.thread_id ORDER BY date_message DESC LIMIT 1) as last_status,
                (SELECT COUNT(*) FROM ANOMALIES_DETAILLEES WHERE thread_id = THREADS.thread_id) as anomaly_count
            FROM THREADS
            ORDER BY date_modification DESC
            LIMIT ? OFFSET ?
        """, (limit, offset))
        
        history_list = []
        for row in cursor.fetchall():
            history_list.append({
                "id": row["thread_id"],
                "title": row["title"],
                "date": row["date_modification"],
                "message_count": row["message_count"],
                "last_status": row["last_status"] or "Succ√®s",
                "anomaly_count": row["anomaly_count"] or 0
            })
            
        cursor.execute("SELECT COUNT(*) as total FROM THREADS")
        total = cursor.fetchone()["total"]
            
        return {
            "history": history_list,
            "pagination": {
                "total": total,
                "limit": limit,
                "offset": offset
            }
        }
    
    except Exception as e:
        logger.error(f"Erreur r√©cup√©ration historique: {e}")
        raise HTTPException(status_code=500, detail="Impossible de charger l'historique.")
    finally:
        if conn:
            conn.close()

@app.get("/api/thread/{thread_id}")
async def get_thread_detail(thread_id: int):
    """R√©cup√®re le d√©tail complet d'un thread."""
    conn = None
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT title FROM THREADS WHERE thread_id = ?", (thread_id,))
        thread_title = cursor.fetchone()
        
        if not thread_title:
            raise HTTPException(status_code=404, detail="Analyse non trouv√©e.")
            
        cursor.execute("""
            SELECT 
                m.message_id, m.sender, m.content, m.date_message, m.statut,
                GROUP_CONCAT(fm.nom_fichier) as fichiers
            FROM MESSAGES m
            LEFT JOIN FICHIERS_MESSAGES fm ON m.message_id = fm.message_id
            WHERE m.thread_id = ?
            GROUP BY m.message_id
            ORDER BY m.date_message ASC
        """, (thread_id,))
        
        messages = []
        for row in cursor.fetchall():
            files = row["fichiers"].split(",") if row["fichiers"] else []
            files = [f.strip() for f in files if f.strip()]
            
            messages.append({
                "id": row["message_id"],
                "sender": row["sender"],
                "content": row["content"],
                "date": row["date_message"],
                "status": row["statut"],
                "files": files
            })
        
        return {
            "id": thread_id,
            "title": thread_title["title"],
            "messages": messages
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur d√©tail thread {thread_id}: {e}")
        raise HTTPException(status_code=500, detail="Erreur lors de la r√©cup√©ration des d√©tails.")
    finally:
        if conn:
            conn.close()

@app.get("/api/thread/{thread_id}/files")
async def get_thread_files(thread_id: int):
    """R√©cup√®re les fichiers et leur contenu pour un thread."""
    conn = None
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT 
                fm.nom_fichier,
                fm.file_content,
                fm.type_mime,
                fm.taille_ko
            FROM FICHIERS_MESSAGES fm
            JOIN MESSAGES m ON fm.message_id = m.message_id
            WHERE m.thread_id = ?
            ORDER BY fm.fichier_message_id
        """, (thread_id,))
        
        files_data = []
        for row in cursor.fetchall():
            files_data.append({
                "filename": row["nom_fichier"],
                "content": row["file_content"],
                "mime_type": row["type_mime"],
                "size_kb": row["taille_ko"]
            })
        
        return {
            "thread_id": thread_id,
            "files": files_data
        }
        
    except Exception as e:
        logger.error(f"Erreur r√©cup√©ration fichiers thread {thread_id}: {e}")
        raise HTTPException(status_code=500, detail="Erreur lors de la r√©cup√©ration des fichiers.")
    finally:
        if conn:
            conn.close()

@app.delete("/api/thread/{thread_id}")
async def delete_thread(thread_id: int):
    """Supprime un thread et tous ses messages associ√©s."""
    conn = None
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("DELETE FROM THREADS WHERE thread_id = ?", (thread_id,))
        
        if cursor.rowcount == 0:
            raise HTTPException(status_code=404, detail="Analyse non trouv√©e.")
        
        conn.commit()
        logger.info(f"Thread {thread_id} supprim√©")
        return {"message": f"Analyse #{thread_id} supprim√©e avec succ√®s."}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur suppression thread {thread_id}: {e}")
        raise HTTPException(status_code=500, detail="Erreur lors de la suppression.")
    finally:
        if conn:
            conn.close()

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)